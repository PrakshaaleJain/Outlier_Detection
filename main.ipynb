{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dtCM--56T4bo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4240e60e-731f-436d-ca9b-6ddc2606009f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "2+3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cardio Dataset"
      ],
      "metadata": {
        "id": "RxDtEpd7Lf7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"CTG.xls\"\n",
        "xls = pd.ExcelFile(file_path)\n",
        "df = pd.read_excel(xls, sheet_name=\"Raw Data\")\n",
        "df = df.dropna(how=\"all\")\n",
        "df = df.drop(columns=[\"FileName\", \"Date\", \"SegFile\"], errors=\"ignore\")\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "df = df[df['NSP'] != 2]  # removing the Suspect class from the cardio dataset\n",
        "df['NSP'] = df['NSP'].astype(int)\n",
        "\n",
        "# Print cleaned dataset\n",
        "print(\"First few rows of the cleaned dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Convert all columns to numeric (except NSP, which is our target)\n",
        "X = df.iloc[:, :-1].values  # Features\n",
        "y = df.iloc[:, -1].values.astype(float)  # Target\n",
        "\n",
        "# Test/train split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "\n",
        "s = int(len(X_train) / 10)\n",
        "X_sample, _, y_sample, _ = train_test_split(X_train, y_train, train_size=s, stratify=y_train, random_state=42)\n",
        "\n",
        "# Print dataset shape\n",
        "print(f\"Final dataset shape: {X.shape}, {y.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qvZq2m0Lfcg",
        "outputId": "f5213ce5-c65a-4684-bb2f-4d0e86235136"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the cleaned dataset:\n",
            "       b       e    LBE     LB   AC   FM    UC  ASTV  MSTV  ALTV  ...    C  \\\n",
            "1    5.0   632.0  132.0  132.0  4.0  0.0   4.0  17.0   2.1   0.0  ...  0.0   \n",
            "2  177.0   779.0  133.0  133.0  2.0  0.0   5.0  16.0   2.1   0.0  ...  0.0   \n",
            "3  411.0  1192.0  134.0  134.0  2.0  0.0   6.0  16.0   2.4   0.0  ...  0.0   \n",
            "4  533.0  1147.0  132.0  132.0  4.0  0.0   5.0  16.0   2.4   0.0  ...  0.0   \n",
            "5    0.0   953.0  134.0  134.0  1.0  0.0  10.0  26.0   5.9   0.0  ...  0.0   \n",
            "\n",
            "     D    E   AD   DE   LD   FS  SUSP  CLASS  NSP  \n",
            "1  0.0  0.0  1.0  0.0  0.0  0.0   0.0    6.0    1  \n",
            "2  0.0  0.0  1.0  0.0  0.0  0.0   0.0    6.0    1  \n",
            "3  0.0  0.0  1.0  0.0  0.0  0.0   0.0    6.0    1  \n",
            "4  0.0  0.0  0.0  0.0  0.0  0.0   0.0    2.0    1  \n",
            "5  0.0  0.0  0.0  0.0  1.0  0.0   0.0    8.0    3  \n",
            "\n",
            "[5 rows x 37 columns]\n",
            "Final dataset shape: (1831, 36), (1831,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RandNet"
      ],
      "metadata": {
        "id": "9bjuegrPLIfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from statistics import median\n",
        "from copy import deepcopy\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "dr3CGTHPX4Nv"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Layer(nn.Module):\n",
        "#     def __init__(self, fan_in, fan_out, connection_prob=1.0, activation=None):\n",
        "#         \"\"\"\n",
        "#         input : number of input nodes\n",
        "#         output: number of output nodes\n",
        "#         \"\"\"\n",
        "#         super(Layer, self).__init__()  # call for constructor\n",
        "#         self.fan_in = fan_in\n",
        "#         self.fan_out = fan_out\n",
        "#         self.activation = activation\n",
        "\n",
        "#         #  creating and intializing weights and biases\n",
        "#         self.weight = nn.Parameter(torch.Tensor(fan_out, fan_in))\n",
        "#         self.bias = nn.Parameter(torch.Tensor(fan_out))\n",
        "\n",
        "#         nn.init.kaiming_uniform_(self.weight)\n",
        "#         nn.init.zeros_(self.bias)   # Setting bias to zero.\n",
        "\n",
        "#         # Generate a fixed binary mask for the weight matrix based on connection_prob.\n",
        "#         self.register_buffer('mask', (torch.rand(fan_out, fan_in) < connection_prob).float())\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         masked_weight = self.weight * self.mask\n",
        "#         out = F.linear(input, masked_weight, self.bias)\n",
        "#         if self.activation:\n",
        "#                 out = self.activation(out)\n",
        "#         return out\n",
        "\n",
        "\n",
        "class Layer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, connection_prob):\n",
        "        super(Layer, self).__init__()\n",
        "        self.connection_mask = (torch.rand(out_features, in_features) < connection_prob).float()\n",
        "        self.weights = nn.Parameter(torch.randn(out_features, in_features) * self.connection_mask)\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.linear(x, self.weights * self.connection_mask, self.bias)"
      ],
      "metadata": {
        "id": "zDsZlJ8BYCyG"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandNet(nn.Module):\n",
        "    def __init__(self, input_dim, num_layers, structure_param=0.5, connection_prob=0.8):\n",
        "        super(RandNet, self).__init__()\n",
        "\n",
        "        if num_layers % 2 == 0:\n",
        "            raise ValueError(\"num_layers should be odd to have a single bottleneck layer.\")\n",
        "\n",
        "        # Build encoder\n",
        "        num_neurons = [input_dim]\n",
        "        for i in range(1, (num_layers // 2) + 1):\n",
        "            next_neurons = max(3, int(num_neurons[-1] * structure_param))\n",
        "            num_neurons.append(next_neurons)\n",
        "\n",
        "        encoder_layers = []\n",
        "        for i in range(len(num_neurons) - 1):\n",
        "            encoder_layers.append(Layer(num_neurons[i], num_neurons[i+1], connection_prob))\n",
        "            encoder_layers.append(nn.Sigmoid() if i == 0 else nn.ReLU())\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # Build decoder\n",
        "        decoder_neurons = list(reversed(num_neurons))\n",
        "        decoder_layers = []\n",
        "        for i in range(len(decoder_neurons) - 1):\n",
        "            decoder_layers.append(Layer(decoder_neurons[i], decoder_neurons[i+1], connection_prob))\n",
        "            decoder_layers.append(nn.ReLU() if i < len(decoder_neurons)-2 else nn.Sigmoid())\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))\n",
        "\n",
        "def train_autoencoder(model, input, epochs, adaptive_factor=1.01, learning_rate=0.01, device='cpu'):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, eps=1e-8, alpha=0.9)\n",
        "    n_samples = input.shape[0]\n",
        "\n",
        "    for i in range(1, epochs + 1):\n",
        "        sample_size = int(min(n_samples, max(10, adaptive_factor ** i)))\n",
        "        indices = np.random.choice(n_samples, sample_size, replace=False)\n",
        "        batch = input[indices].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)\n",
        "        loss = F.mse_loss(output, batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            print(f\"Iteration {i}/{epochs}, Loss: {loss.item():.6f}\")\n",
        "    return model\n",
        "\n",
        "def compute_reconstruction_loss(model, input, device='cpu'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input = input.to(device)\n",
        "        output = model(input)\n",
        "        loss = torch.sum((input - output) ** 2, dim=1)\n",
        "    return loss.cpu().numpy()\n",
        "\n",
        "def calculate_outlier_score(ensemble, input, device='cpu'):\n",
        "    all_scores = []\n",
        "    for model in ensemble:\n",
        "        losses = compute_reconstruction_loss(model, input, device)\n",
        "        if losses.std() > 0:\n",
        "            norm_loss = (losses - losses.mean()) / losses.std()\n",
        "        else:\n",
        "            norm_loss = losses\n",
        "        all_scores.append(norm_loss)\n",
        "    all_scores = np.array(all_scores)\n",
        "    final_scores = np.median(all_scores, axis=0)\n",
        "    return final_scores\n",
        "\n",
        "\n",
        "def train_ensemble(input_data, num_models=100, epochs=300, adaptive_factor=1.01,\n",
        "                   structure_param=0.5, num_layers=7, connection_prob=0.8,\n",
        "                   learning_rate=0.01, device='cpu'):\n",
        "    n_samples = input_data.shape[0]\n",
        "    ensemble = []\n",
        "    subsample_size = max(10, n_samples // 10)\n",
        "\n",
        "    for i in range(num_models):\n",
        "        indices = np.random.choice(n_samples, subsample_size, replace=False)\n",
        "        data_subset = input_data[indices]\n",
        "\n",
        "        model = RandNet(input_dim=input_data.shape[1],\n",
        "                        num_layers=num_layers,\n",
        "                        structure_param=structure_param,\n",
        "                        connection_prob=connection_prob)\n",
        "\n",
        "        print(f\"Training model {i + 1}/{num_models}...\")\n",
        "        trained_model = train_autoencoder(model, data_subset, epochs,\n",
        "                                          adaptive_factor=adaptive_factor,\n",
        "                                          learning_rate=learning_rate,\n",
        "                                          device=device)\n",
        "        ensemble.append(deepcopy(trained_model))\n",
        "    return ensemble\n"
      ],
      "metadata": {
        "id": "yK9WJHrxT73K"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "y_test = (y_test != 1).astype(int)  # Treat class \"1\" as inlier, others as outlier (e.g. '2')"
      ],
      "metadata": {
        "id": "9UAE7WCJY0CB"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble = train_ensemble(\n",
        "    input_data=X_train_tensor,\n",
        "    num_models=10,               # Start with 10 for speed; increase later if needed\n",
        "    epochs=1000,\n",
        "    adaptive_factor=1.01,\n",
        "    structure_param=0.5,\n",
        "    num_layers=7,\n",
        "    connection_prob=0.8,\n",
        "    learning_rate=0.01,\n",
        "    device='cpu'  # or 'cuda' if using GPU\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmgwWUCsnxv_",
        "outputId": "20b19241-b6c3-43d9-ff74-4754bd1c2607",
        "collapsed": true
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model 1/10...\n",
            "Iteration 200/1000, Loss: 0.039993\n",
            "Iteration 400/1000, Loss: 0.043923\n",
            "Iteration 600/1000, Loss: 0.044185\n",
            "Iteration 800/1000, Loss: 0.044166\n",
            "Iteration 1000/1000, Loss: 0.044164\n",
            "Training model 2/10...\n",
            "Iteration 200/1000, Loss: 0.046059\n",
            "Iteration 400/1000, Loss: 0.043322\n",
            "Iteration 600/1000, Loss: 0.045531\n",
            "Iteration 800/1000, Loss: 0.045541\n",
            "Iteration 1000/1000, Loss: 0.045535\n",
            "Training model 3/10...\n",
            "Iteration 200/1000, Loss: 0.047845\n",
            "Iteration 400/1000, Loss: 0.045260\n",
            "Iteration 600/1000, Loss: 0.045271\n",
            "Iteration 800/1000, Loss: 0.045275\n",
            "Iteration 1000/1000, Loss: 0.045274\n",
            "Training model 4/10...\n",
            "Iteration 200/1000, Loss: 0.037986\n",
            "Iteration 400/1000, Loss: 0.025377\n",
            "Iteration 600/1000, Loss: 0.020514\n",
            "Iteration 800/1000, Loss: 0.017557\n",
            "Iteration 1000/1000, Loss: 0.018242\n",
            "Training model 5/10...\n",
            "Iteration 200/1000, Loss: 0.031451\n",
            "Iteration 400/1000, Loss: 0.021235\n",
            "Iteration 600/1000, Loss: 0.017533\n",
            "Iteration 800/1000, Loss: 0.016220\n",
            "Iteration 1000/1000, Loss: 0.015119\n",
            "Training model 6/10...\n",
            "Iteration 200/1000, Loss: 0.051429\n",
            "Iteration 400/1000, Loss: 0.045425\n",
            "Iteration 600/1000, Loss: 0.045866\n",
            "Iteration 800/1000, Loss: 0.045862\n",
            "Iteration 1000/1000, Loss: 0.045874\n",
            "Training model 7/10...\n",
            "Iteration 200/1000, Loss: 0.047291\n",
            "Iteration 400/1000, Loss: 0.036449\n",
            "Iteration 600/1000, Loss: 0.028780\n",
            "Iteration 800/1000, Loss: 0.025887\n",
            "Iteration 1000/1000, Loss: 0.023522\n",
            "Training model 8/10...\n",
            "Iteration 200/1000, Loss: 0.030020\n",
            "Iteration 400/1000, Loss: 0.021440\n",
            "Iteration 600/1000, Loss: 0.016998\n",
            "Iteration 800/1000, Loss: 0.014523\n",
            "Iteration 1000/1000, Loss: 0.013318\n",
            "Training model 9/10...\n",
            "Iteration 200/1000, Loss: 0.053235\n",
            "Iteration 400/1000, Loss: 0.051811\n",
            "Iteration 600/1000, Loss: 0.048900\n",
            "Iteration 800/1000, Loss: 0.048911\n",
            "Iteration 1000/1000, Loss: 0.048900\n",
            "Training model 10/10...\n",
            "Iteration 200/1000, Loss: 0.048087\n",
            "Iteration 400/1000, Loss: 0.042524\n",
            "Iteration 600/1000, Loss: 0.046231\n",
            "Iteration 800/1000, Loss: 0.046281\n",
            "Iteration 1000/1000, Loss: 0.046237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "outlier_scores = calculate_outlier_score(ensemble, X_test_tensor, device='cpu')\n",
        "# Assuming y_test (binary labels: 0 for normal, 1 for anomaly) and outlier_scores are computed\n",
        "auc_score = roc_auc_score(y_test, outlier_scores)\n",
        "print(f\"Final AUC Score on test data by RandNet: {auc_score:.4f}\")"
      ],
      "metadata": {
        "id": "qKN5A7RBoCQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b96062d-39c1-4d23-c6b0-1e9fde20b3c2"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final AUC Score on test data by RandNet: 0.9817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOF"
      ],
      "metadata": {
        "id": "hIrUTpqaLDIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.metrics import roc_auc_score\n"
      ],
      "metadata": {
        "id": "sZ3VHDE5KIJ3"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lof = LocalOutlierFactor(n_neighbors=20, novelty=False)\n",
        "y_pred_lof = lof.fit_predict(X_test)\n",
        "lof_scores = -lof.negative_outlier_factor_"
      ],
      "metadata": {
        "id": "2Mjv138pLNeT"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc = roc_auc_score(y_test, lof_scores)\n",
        "print(f\"AUC Score (LOF): {auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag625YyRLk0g",
        "outputId": "5aa70cad-1e56-4982-9966-d52c63ddc413"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score (LOF): 0.6355\n"
          ]
        }
      ]
    }
  ]
}